{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2372, 0.2246, 0.5621, 0.8302],\n",
      "        [0.8773, 0.8323, 0.4185, 0.4526],\n",
      "        [0.9798, 0.6827, 0.6646, 0.2128],\n",
      "        [0.9298, 0.8947, 0.8593, 0.8535]])\n"
     ]
    }
   ],
   "source": [
    "random_torch = torch.rand(4, 4)\n",
    "print(random_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        '''将输入的词汇表索引转换为指定维度的Embedding, d_model为模型维度'''\n",
    "        super().__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, max_len, d_model, device):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device = device)\n",
    "        self.encoding.requires_grad = False\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim = 1) # 转换为二维张量\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        # 根据Transformer论文公式\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (1000 ** (_2i / d_model))) # [:, 0::2]表示选择所有行，取每行中的偶数索引元素，计算sin值作为位置编码\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (1000 ** (_2i / d_model)))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :] # 返回编码矩阵中前seq_len行的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(max_len, d_model, device)\n",
    "        self.drop_out = torch.nn.Dropout(p = drop_prob) # 在训练过程中随机丢弃一些神经元以减少过拟合\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_embedding = self.token_embedding(x) # 实际上是调用了torch.nn.Embedding的__call__()，执行前向传播\n",
    "        positional_embedding = self.positional_embedding(x)\n",
    "        return self.drop_out(token_embedding + positional_embedding)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
